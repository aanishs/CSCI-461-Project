{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Time Series Model for Tic Intensity Prediction\n",
    "\n",
    "**Goal**: Predict the intensity of the next tic based on previous tic history\n",
    "\n",
    "**Approach**: Sequence-based prediction using sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, f1_score, precision_recall_curve, auc, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('results (2).csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA CLEANING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Parse timestamps\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Convert intensity to numeric\n",
    "df['intensity'] = pd.to_numeric(df['intensity'], errors='coerce')\n",
    "\n",
    "# Handle 'null' strings\n",
    "for col in ['mood', 'trigger']:\n",
    "    df[col] = df[col].replace('null', np.nan)\n",
    "\n",
    "# Sort by user and timestamp (CRITICAL for time series)\n",
    "df = df.sort_values(['userId', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTotal records: {len(df)}\")\n",
    "print(f\"Unique users: {df['userId'].nunique()}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df[['intensity', 'type', 'timeOfDay', 'mood', 'trigger']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick EDA on intensity\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Intensity distribution\n",
    "axes[0].hist(df['intensity'].dropna(), bins=10, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Intensity')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Intensity Distribution')\n",
    "axes[0].axvline(df['intensity'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"intensity\"].mean():.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# High vs low intensity\n",
    "df['high_intensity'] = (df['intensity'] >= 7).astype(int)\n",
    "high_count = df['high_intensity'].sum()\n",
    "low_count = len(df) - high_count\n",
    "axes[1].bar(['Low (<7)', 'High (â‰¥7)'], [low_count, high_count], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title(f'Binary Classification Target\\n(High: {high_count/len(df)*100:.1f}%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nIntensity Statistics:\")\n",
    "print(f\"  Mean: {df['intensity'].mean():.2f}\")\n",
    "print(f\"  Median: {df['intensity'].median():.1f}\")\n",
    "print(f\"  Std: {df['intensity'].std():.2f}\")\n",
    "print(f\"  High-intensity rate: {high_count/len(df)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"FEATURE ENGINEERING\")\nprint(\"=\" * 60)\n\n# Extract temporal features\ndf['hour'] = df['timestamp'].dt.hour\ndf['day_of_week'] = df['timestamp'].dt.dayofweek  # 0=Monday, 6=Sunday\ndf['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n\n# Calculate time since previous tic (per user)\ndf['time_since_prev_hours'] = df.groupby('userId')['timestamp'].diff().dt.total_seconds() / 3600\n\n# Calculate user-level statistics (up to current tic)\ndf['user_mean_intensity'] = df.groupby('userId')['intensity'].expanding().mean().shift(1).reset_index(level=0, drop=True)\ndf['user_std_intensity'] = df.groupby('userId')['intensity'].expanding().std().shift(1).reset_index(level=0, drop=True)\ndf['user_tic_count'] = df.groupby('userId').cumcount()  # Number of tics so far\n\n# Lag features (previous tic intensity)\ndf['prev_intensity_1'] = df.groupby('userId')['intensity'].shift(1)\ndf['prev_intensity_2'] = df.groupby('userId')['intensity'].shift(2)\ndf['prev_intensity_3'] = df.groupby('userId')['intensity'].shift(3)\n\n# Previous tic type\ndf['prev_type_1'] = df.groupby('userId')['type'].shift(1)\n\n# Encode categorical features\nle_type = LabelEncoder()\nle_prev_type = LabelEncoder()\nle_time_of_day = LabelEncoder()\n\ndf['type_encoded'] = le_type.fit_transform(df['type'].fillna('unknown'))\ndf['prev_type_encoded'] = le_prev_type.fit_transform(df['prev_type_1'].fillna('unknown'))\ndf['timeOfDay_encoded'] = le_time_of_day.fit_transform(df['timeOfDay'].fillna('unknown'))\n\n# Mood and trigger (optional - only when available)\ndf['has_mood'] = (~df['mood'].isna()).astype(int)\ndf['has_trigger'] = (~df['trigger'].isna()).astype(int)\n\n# Encode mood and trigger with 'missing' category\nle_mood = LabelEncoder()\nle_trigger = LabelEncoder()\ndf['mood_encoded'] = le_mood.fit_transform(df['mood'].fillna('missing'))\ndf['trigger_encoded'] = le_trigger.fit_transform(df['trigger'].fillna('missing'))\n\nprint(\"\\nFeatures created:\")\nprint(\"  - Temporal: hour, day_of_week, is_weekend\")\nprint(\"  - Sequence: prev_intensity_1/2/3, time_since_prev_hours\")\nprint(\"  - User-level: user_mean_intensity, user_std_intensity, user_tic_count\")\nprint(\"  - Categorical: type, timeOfDay, prev_type (encoded)\")\nprint(\"  - Optional: mood, trigger (when available)\")\n\ndf.head(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Training Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CREATING TRAINING SEQUENCES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Select features for modeling\n",
    "feature_cols = [\n",
    "    # Temporal\n",
    "    'hour', 'day_of_week', 'is_weekend', 'timeOfDay_encoded',\n",
    "    # Sequence features\n",
    "    'prev_intensity_1', 'prev_intensity_2', 'prev_intensity_3',\n",
    "    'time_since_prev_hours',\n",
    "    # Type features\n",
    "    'type_encoded', 'prev_type_encoded',\n",
    "    # User-level\n",
    "    'user_mean_intensity', 'user_std_intensity', 'user_tic_count',\n",
    "    # Optional\n",
    "    'mood_encoded', 'trigger_encoded', 'has_mood', 'has_trigger'\n",
    "]\n",
    "\n",
    "# Target variables\n",
    "target_regression = 'intensity'\n",
    "target_classification = 'high_intensity'\n",
    "\n",
    "# Filter: only keep rows where we have at least 3 previous tics (for sequence)\n",
    "# This means dropping first 3 tics per user\n",
    "df_model = df[df['user_tic_count'] >= 3].copy()\n",
    "\n",
    "# Remove rows with missing target or critical features\n",
    "df_model = df_model.dropna(subset=[target_regression] + ['prev_intensity_1', 'prev_intensity_2', 'prev_intensity_3'])\n",
    "\n",
    "# Fill remaining missing values\n",
    "df_model['time_since_prev_hours'] = df_model['time_since_prev_hours'].fillna(df_model['time_since_prev_hours'].median())\n",
    "df_model['user_std_intensity'] = df_model['user_std_intensity'].fillna(0)\n",
    "\n",
    "print(f\"\\nOriginal dataset: {len(df)} records\")\n",
    "print(f\"After filtering (â‰¥3 prev tics): {len(df_model)} records\")\n",
    "print(f\"Users in modeling dataset: {df_model['userId'].nunique()}\")\n",
    "print(f\"\\nHigh-intensity rate in model data: {df_model['high_intensity'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-grouped train/test split (80/20)\n",
    "# Important: split by user to avoid data leakage\n",
    "users = df_model['userId'].unique()\n",
    "train_users, test_users = train_test_split(users, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = df_model[df_model['userId'].isin(train_users)]\n",
    "test_df = df_model[df_model['userId'].isin(test_users)]\n",
    "\n",
    "# Also create temporal split within each user (alternative evaluation)\n",
    "# Use first 80% of each user's tics for training\n",
    "def temporal_split(group, train_frac=0.8):\n",
    "    n_train = int(len(group) * train_frac)\n",
    "    group['temporal_split'] = ['train'] * n_train + ['test'] * (len(group) - n_train)\n",
    "    return group\n",
    "\n",
    "df_model = df_model.groupby('userId', group_keys=False).apply(temporal_split)\n",
    "train_temporal = df_model[df_model['temporal_split'] == 'train']\n",
    "test_temporal = df_model[df_model['temporal_split'] == 'test']\n",
    "\n",
    "print(f\"\\nUser-grouped split:\")\n",
    "print(f\"  Train: {len(train_df)} samples, {len(train_users)} users\")\n",
    "print(f\"  Test: {len(test_df)} samples, {len(test_users)} users\")\n",
    "\n",
    "print(f\"\\nTemporal split (within-user):\")\n",
    "print(f\"  Train: {len(train_temporal)} samples\")\n",
    "print(f\"  Test: {len(test_temporal)} samples\")\n",
    "\n",
    "# Prepare features and targets\n",
    "X_train = train_df[feature_cols]\n",
    "y_train_reg = train_df[target_regression]\n",
    "y_train_clf = train_df[target_classification]\n",
    "\n",
    "X_test = test_df[feature_cols]\n",
    "y_test_reg = test_df[target_regression]\n",
    "y_test_clf = test_df[target_classification]\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X_train.shape}\")\n",
    "print(f\"\\nFeatures used: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Regression Model (Predict Exact Intensity 1-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"REGRESSION MODEL: Predict Intensity (1-10)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_reg.fit(X_train, y_train_reg)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = rf_reg.predict(X_train)\n",
    "y_pred_test = rf_reg.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "train_mae = mean_absolute_error(y_train_reg, y_pred_train)\n",
    "test_mae = mean_absolute_error(y_test_reg, y_pred_test)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_reg, y_pred_train))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_test))\n",
    "\n",
    "print(f\"\\nRandom Forest Regressor Results:\")\n",
    "print(f\"  Train MAE: {train_mae:.3f}\")\n",
    "print(f\"  Test MAE:  {test_mae:.3f}\")\n",
    "print(f\"  Train RMSE: {train_rmse:.3f}\")\n",
    "print(f\"  Test RMSE:  {test_rmse:.3f}\")\n",
    "\n",
    "# Baseline comparison (predict mean intensity)\n",
    "baseline_pred = np.full(len(y_test_reg), y_train_reg.mean())\n",
    "baseline_mae = mean_absolute_error(y_test_reg, baseline_pred)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test_reg, baseline_pred))\n",
    "\n",
    "print(f\"\\nBaseline (predict mean) Results:\")\n",
    "print(f\"  Baseline MAE: {baseline_mae:.3f}\")\n",
    "print(f\"  Baseline RMSE: {baseline_rmse:.3f}\")\n",
    "print(f\"\\nImprovement over baseline:\")\n",
    "print(f\"  MAE improvement: {(baseline_mae - test_mae) / baseline_mae * 100:.1f}%\")\n",
    "print(f\"  RMSE improvement: {(baseline_rmse - test_rmse) / baseline_rmse * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actuals\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test_reg, y_pred_test, alpha=0.5, s=20)\n",
    "axes[0].plot([1, 10], [1, 10], 'r--', label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual Intensity')\n",
    "axes[0].set_ylabel('Predicted Intensity')\n",
    "axes[0].set_title(f'Regression: Predicted vs Actual\\n(Test MAE: {test_mae:.3f})')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test_reg - y_pred_test\n",
    "axes[1].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(0, color='red', linestyle='--', label='Zero error')\n",
    "axes[1].set_xlabel('Prediction Error (Actual - Predicted)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Residual Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Binary Classification Model (High â‰¥7 vs Low <7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BINARY CLASSIFICATION: High (â‰¥7) vs Low (<7)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_clf.fit(X_train, y_train_clf)\n",
    "\n",
    "# Predictions\n",
    "y_pred_clf = rf_clf.predict(X_test)\n",
    "y_pred_proba = rf_clf.predict_proba(X_test)[:, 1]  # Probability of high intensity\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "accuracy = accuracy_score(y_test_clf, y_pred_clf)\n",
    "precision = precision_score(y_test_clf, y_pred_clf)\n",
    "recall = recall_score(y_test_clf, y_pred_clf)\n",
    "f1 = f1_score(y_test_clf, y_pred_clf)\n",
    "\n",
    "print(f\"\\nRandom Forest Classifier Results:\")\n",
    "print(f\"  Accuracy:  {accuracy:.3f}\")\n",
    "print(f\"  Precision: {precision:.3f}\")\n",
    "print(f\"  Recall:    {recall:.3f}\")\n",
    "print(f\"  F1-score:  {f1:.3f}\")\n",
    "\n",
    "# Calculate PR-AUC (important for imbalanced data)\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test_clf, y_pred_proba)\n",
    "pr_auc = auc(recall_curve, precision_curve)\n",
    "print(f\"  PR-AUC:    {pr_auc:.3f}\")\n",
    "\n",
    "# Baseline (always predict majority class)\n",
    "baseline_acc = (y_test_clf == 0).mean()  # Accuracy if we always predict \"low\"\n",
    "print(f\"\\nBaseline (predict majority class):\")\n",
    "print(f\"  Baseline Accuracy: {baseline_acc:.3f}\")\n",
    "print(f\"  Model improvement: {(accuracy - baseline_acc) / baseline_acc * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_clf, y_pred_clf, target_names=['Low (<7)', 'High (â‰¥7)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Precision-Recall curve\n",
    "axes[0].plot(recall_curve, precision_curve, label=f'PR-AUC = {pr_auc:.3f}')\n",
    "axes[0].set_xlabel('Recall')\n",
    "axes[0].set_ylabel('Precision')\n",
    "axes[0].set_title('Precision-Recall Curve')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test_clf, y_pred_clf)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance_reg = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_reg.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "feature_importance_clf = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_clf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Regression\n",
    "top_features_reg = feature_importance_reg.head(15)\n",
    "axes[0].barh(range(len(top_features_reg)), top_features_reg['importance'], alpha=0.7)\n",
    "axes[0].set_yticks(range(len(top_features_reg)))\n",
    "axes[0].set_yticklabels(top_features_reg['feature'])\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Top 15 Features - Regression Model')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Classification\n",
    "top_features_clf = feature_importance_clf.head(15)\n",
    "axes[1].barh(range(len(top_features_clf)), top_features_clf['importance'], alpha=0.7, color='orange')\n",
    "axes[1].set_yticks(range(len(top_features_clf)))\n",
    "axes[1].set_yticklabels(top_features_clf['feature'])\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Top 15 Features - Classification Model')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Features - Regression:\")\n",
    "print(feature_importance_reg.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 10 Features - Classification:\")\n",
    "print(feature_importance_clf.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Per-User Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PER-USER PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate MAE per user in test set\n",
    "test_df_with_pred = test_df.copy()\n",
    "test_df_with_pred['pred_intensity'] = y_pred_test\n",
    "test_df_with_pred['abs_error'] = np.abs(test_df_with_pred['intensity'] - test_df_with_pred['pred_intensity'])\n",
    "\n",
    "user_performance = test_df_with_pred.groupby('userId').agg({\n",
    "    'abs_error': 'mean',\n",
    "    'intensity': ['count', 'mean', 'std']\n",
    "}).round(3)\n",
    "\n",
    "user_performance.columns = ['mae', 'n_samples', 'mean_intensity', 'std_intensity']\n",
    "user_performance = user_performance.sort_values('mae')\n",
    "\n",
    "print(f\"\\nPer-user MAE statistics:\")\n",
    "print(f\"  Mean MAE across users: {user_performance['mae'].mean():.3f}\")\n",
    "print(f\"  Median MAE: {user_performance['mae'].median():.3f}\")\n",
    "print(f\"  Best user MAE: {user_performance['mae'].min():.3f}\")\n",
    "print(f\"  Worst user MAE: {user_performance['mae'].max():.3f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MAE distribution across users\n",
    "axes[0].hist(user_performance['mae'], bins=15, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(user_performance['mae'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0].set_xlabel('MAE per User')\n",
    "axes[0].set_ylabel('Number of Users')\n",
    "axes[0].set_title('Distribution of MAE Across Users')\n",
    "axes[0].legend()\n",
    "\n",
    "# MAE vs number of samples\n",
    "axes[1].scatter(user_performance['n_samples'], user_performance['mae'], alpha=0.6)\n",
    "axes[1].set_xlabel('Number of Test Samples per User')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('MAE vs Sample Size\\n(Do we perform better with more data?)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop 5 best-predicted users:\")\n",
    "print(user_performance.head(5))\n",
    "\n",
    "print(f\"\\nTop 5 worst-predicted users:\")\n",
    "print(user_performance.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BASELINE MODEL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset:\")\n",
    "print(f\"   - Total usable sequences: {len(df_model)}\")\n",
    "print(f\"   - Users: {df_model['userId'].nunique()}\")\n",
    "print(f\"   - Features: {len(feature_cols)}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Regression Performance (Predict Intensity 1-10):\")\n",
    "print(f\"   - Test MAE: {test_mae:.3f}\")\n",
    "print(f\"   - Test RMSE: {test_rmse:.3f}\")\n",
    "print(f\"   - Improvement over baseline: {(baseline_mae - test_mae) / baseline_mae * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Classification Performance (High â‰¥7 vs Low <7):\")\n",
    "print(f\"   - Test Accuracy: {accuracy:.3f}\")\n",
    "print(f\"   - F1-score: {f1:.3f}\")\n",
    "print(f\"   - PR-AUC: {pr_auc:.3f}\")\n",
    "print(f\"   - Precision: {precision:.3f}\")\n",
    "print(f\"   - Recall: {recall:.3f}\")\n",
    "\n",
    "print(f\"\\nðŸ”‘ Top 3 Most Important Features (Regression):\")\n",
    "for i, row in feature_importance_reg.head(3).iterrows():\n",
    "    print(f\"   {i+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Next Steps:\")\n",
    "print(f\"   1. Try XGBoost for potentially better performance\")\n",
    "print(f\"   2. Experiment with deeper sequence models (LSTM)\")\n",
    "print(f\"   3. Build user-specific models for high-engagement users\")\n",
    "print(f\"   4. Implement cold-start strategy for new users\")\n",
    "print(f\"   5. Test different prediction windows (3-day, 7-day aggregates)\")\n",
    "print(f\"   6. Add more feature engineering (interaction terms, polynomial features)\")\n",
    "print(f\"   7. Hyperparameter tuning with grid search\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}