{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search Results Analysis\n",
    "\n",
    "This notebook analyzes the results from the hyperparameter search experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd() / 'src'))\n",
    "\n",
    "from experiment_tracker import ExperimentTracker\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tracker\n",
    "tracker = ExperimentTracker(experiment_dir='experiments')\n",
    "\n",
    "# Print summary\n",
    "tracker.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all results\n",
    "results = tracker.load_results()\n",
    "\n",
    "print(f\"Total experiments: {len(results)}\")\n",
    "print(f\"\\nColumns: {results.columns.tolist()}\")\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overall Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best regression models (by Test MAE)\n",
    "print(\"=\"*80)\n",
    "print(\"TOP 10 REGRESSION MODELS (by Test MAE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter for regression targets\n",
    "regression_cols = [c for c in results.columns if 'intensity' in str(c).lower() or 'count' in str(c).lower() or 'time' in str(c).lower()]\n",
    "reg_results = results[results['target_type'].str.contains('intensity|count|time', na=False)]\n",
    "\n",
    "if 'metric_test_mae' in reg_results.columns and len(reg_results) > 0:\n",
    "    top_reg = reg_results.nsmallest(10, 'metric_test_mae')\n",
    "    display_cols = ['run_id', 'model_name', 'target_type', 'metric_test_mae', 'metric_test_rmse']\n",
    "    display_cols = [c for c in display_cols if c in top_reg.columns]\n",
    "    print(top_reg[display_cols].to_string(index=False))\n",
    "else:\n",
    "    print(\"No regression results found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best classification models (by Test F1)\n",
    "print(\"=\"*80)\n",
    "print(\"TOP 10 CLASSIFICATION MODELS (by Test F1)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "clf_results = results[results['target_type'].str.contains('high', na=False)]\n",
    "\n",
    "if 'metric_test_f1' in clf_results.columns and len(clf_results) > 0:\n",
    "    top_clf = clf_results.nlargest(10, 'metric_test_f1')\n",
    "    display_cols = ['run_id', 'model_name', 'target_type', 'metric_test_f1', 'metric_test_precision', 'metric_test_recall']\n",
    "    display_cols = [c for c in display_cols if c in top_clf.columns]\n",
    "    print(top_clf[display_cols].to_string(index=False))\n",
    "else:\n",
    "    print(\"No classification results found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models across different targets\n",
    "if 'model_name' in results.columns and 'target_type' in results.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Regression: MAE by model and target\n",
    "    if 'metric_test_mae' in reg_results.columns and len(reg_results) > 0:\n",
    "        sns.boxplot(data=reg_results, x='model_name', y='metric_test_mae', ax=axes[0])\n",
    "        axes[0].set_title('Test MAE by Model (Regression Tasks)', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('Model')\n",
    "        axes[0].set_ylabel('Test MAE')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Classification: F1 by model\n",
    "    if 'metric_test_f1' in clf_results.columns and len(clf_results) > 0:\n",
    "        sns.boxplot(data=clf_results, x='model_name', y='metric_test_f1', ax=axes[1])\n",
    "        axes[1].set_title('Test F1 by Model (Classification Tasks)', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Model')\n",
    "        axes[1].set_ylabel('Test F1')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze impact of key hyperparameters on performance\n",
    "# Example: n_estimators for Random Forest\n",
    "\n",
    "rf_results = results[results['model_name'] == 'random_forest']\n",
    "\n",
    "if len(rf_results) > 0 and 'config_n_estimators' in rf_results.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # MAE vs n_estimators\n",
    "    rf_reg = rf_results[rf_results['target_type'].str.contains('intensity', na=False)]\n",
    "    if len(rf_reg) > 0 and 'metric_test_mae' in rf_reg.columns:\n",
    "        sns.scatterplot(data=rf_reg, x='config_n_estimators', y='metric_test_mae', \n",
    "                       hue='target_type', ax=axes[0], alpha=0.6)\n",
    "        axes[0].set_title('Random Forest: MAE vs n_estimators', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('n_estimators')\n",
    "        axes[0].set_ylabel('Test MAE')\n",
    "    \n",
    "    # MAE vs max_depth\n",
    "    if 'config_max_depth' in rf_reg.columns:\n",
    "        rf_reg_depth = rf_reg[rf_reg['config_max_depth'].notna()]\n",
    "        if len(rf_reg_depth) > 0:\n",
    "            sns.scatterplot(data=rf_reg_depth, x='config_max_depth', y='metric_test_mae',\n",
    "                           hue='target_type', ax=axes[1], alpha=0.6)\n",
    "            axes[1].set_title('Random Forest: MAE vs max_depth', fontsize=14, fontweight='bold')\n",
    "            axes[1].set_xlabel('max_depth')\n",
    "            axes[1].set_ylabel('Test MAE')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No Random Forest results found for hyperparameter analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Set Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance across different feature sets\n",
    "if 'config_feature_set' in results.columns:\n",
    "    feature_set_results = results[results['config_feature_set'].notna()]\n",
    "    \n",
    "    if len(feature_set_results) > 0:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Regression\n",
    "        fs_reg = feature_set_results[feature_set_results['target_type'].str.contains('intensity', na=False)]\n",
    "        if len(fs_reg) > 0 and 'metric_test_mae' in fs_reg.columns:\n",
    "            sns.boxplot(data=fs_reg, x='config_feature_set', y='metric_test_mae', ax=axes[0])\n",
    "            axes[0].set_title('Test MAE by Feature Set', fontsize=14, fontweight='bold')\n",
    "            axes[0].set_xlabel('Feature Set')\n",
    "            axes[0].set_ylabel('Test MAE')\n",
    "            axes[0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Classification\n",
    "        fs_clf = feature_set_results[feature_set_results['target_type'].str.contains('high', na=False)]\n",
    "        if len(fs_clf) > 0 and 'metric_test_f1' in fs_clf.columns:\n",
    "            sns.boxplot(data=fs_clf, x='config_feature_set', y='metric_test_f1', ax=axes[1])\n",
    "            axes[1].set_title('Test F1 by Feature Set', fontsize=14, fontweight='bold')\n",
    "            axes[1].set_xlabel('Feature Set')\n",
    "            axes[1].set_ylabel('Test F1')\n",
    "            axes[1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No feature set comparison data available.\")\n",
    "else:\n",
    "    print(\"Feature set information not found in results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction Window Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does performance vary with prediction window (k days)?\n",
    "# Extract k from target_type (e.g., target_high_count_next_7d -> 7)\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_k_days(target):\n",
    "    match = re.search(r'(\\d+)d', str(target))\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "results['k_days'] = results['target_type'].apply(extract_k_days)\n",
    "\n",
    "k_results = results[results['k_days'].notna()]\n",
    "\n",
    "if len(k_results) > 0 and 'metric_test_mae' in k_results.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=k_results, x='k_days', y='metric_test_mae', \n",
    "                 hue='model_name', marker='o', ci=95)\n",
    "    plt.title('Test MAE vs Prediction Window (k days)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Prediction Window (k days)')\n",
    "    plt.ylabel('Test MAE')\n",
    "    plt.legend(title='Model')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No prediction window data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Target Type Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance across different prediction targets\n",
    "if 'target_type' in results.columns:\n",
    "    target_summary = results.groupby('target_type').agg({\n",
    "        'metric_test_mae': ['mean', 'std', 'min'],\n",
    "        'metric_test_f1': ['mean', 'std', 'max'],\n",
    "        'run_id': 'count'\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"PERFORMANCE BY TARGET TYPE\")\n",
    "    print(\"=\"*80)\n",
    "    print(target_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze training time vs performance trade-offs\n",
    "if 'metric_train_time_seconds' in results.columns and 'metric_test_mae' in results.columns:\n",
    "    time_results = results[results['metric_train_time_seconds'].notna()]\n",
    "    \n",
    "    if len(time_results) > 0:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(data=time_results, x='metric_train_time_seconds', y='metric_test_mae',\n",
    "                       hue='model_name', size='config_n_train_samples', alpha=0.6)\n",
    "        plt.title('Training Time vs Test MAE', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Training Time (seconds)')\n",
    "        plt.ylabel('Test MAE')\n",
    "        plt.xscale('log')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Training time data not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Best Models for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export top 5 models for each target type\n",
    "best_models = []\n",
    "\n",
    "for target in results['target_type'].unique():\n",
    "    target_results = results[results['target_type'] == target]\n",
    "    \n",
    "    # Determine metric based on task type\n",
    "    if 'high' in str(target).lower() and 'count' not in str(target).lower():\n",
    "        # Classification\n",
    "        if 'metric_test_f1' in target_results.columns:\n",
    "            best = target_results.nlargest(5, 'metric_test_f1')\n",
    "            best_models.append(best)\n",
    "    else:\n",
    "        # Regression\n",
    "        if 'metric_test_mae' in target_results.columns:\n",
    "            best = target_results.nsmallest(5, 'metric_test_mae')\n",
    "            best_models.append(best)\n",
    "\n",
    "if best_models:\n",
    "    best_models_df = pd.concat(best_models)\n",
    "    best_models_df.to_csv('experiments/best_models.csv', index=False)\n",
    "    print(f\"Exported {len(best_models_df)} best models to experiments/best_models.csv\")\n",
    "    print(f\"\\nBest models:\")\n",
    "    display_cols = ['run_id', 'model_name', 'target_type', 'metric_test_mae', 'metric_test_f1']\n",
    "    display_cols = [c for c in display_cols if c in best_models_df.columns]\n",
    "    print(best_models_df[display_cols].to_string(index=False))\n",
    "else:\n",
    "    print(\"No best models to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RECOMMENDATIONS BASED ON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(results) > 0:\n",
    "    # Best overall model\n",
    "    if 'metric_test_mae' in reg_results.columns and len(reg_results) > 0:\n",
    "        best_reg = reg_results.loc[reg_results['metric_test_mae'].idxmin()]\n",
    "        print(f\"\\n‚úÖ Best Regression Model:\")\n",
    "        print(f\"   Model: {best_reg['model_name']}\")\n",
    "        print(f\"   Target: {best_reg['target_type']}\")\n",
    "        print(f\"   Test MAE: {best_reg['metric_test_mae']:.4f}\")\n",
    "        if 'config_n_estimators' in best_reg:\n",
    "            print(f\"   Key params: n_estimators={best_reg.get('config_n_estimators', 'N/A')}, \"\n",
    "                  f\"max_depth={best_reg.get('config_max_depth', 'N/A')}\")\n",
    "    \n",
    "    if 'metric_test_f1' in clf_results.columns and len(clf_results) > 0:\n",
    "        best_clf = clf_results.loc[clf_results['metric_test_f1'].idxmax()]\n",
    "        print(f\"\\n‚úÖ Best Classification Model:\")\n",
    "        print(f\"   Model: {best_clf['model_name']}\")\n",
    "        print(f\"   Target: {best_clf['target_type']}\")\n",
    "        print(f\"   Test F1: {best_clf['metric_test_f1']:.4f}\")\n",
    "        if 'config_n_estimators' in best_clf:\n",
    "            print(f\"   Key params: n_estimators={best_clf.get('config_n_estimators', 'N/A')}, \"\n",
    "                  f\"max_depth={best_clf.get('config_max_depth', 'N/A')}\")\n",
    "    \n",
    "    # Feature set recommendation\n",
    "    if 'config_feature_set' in results.columns:\n",
    "        fs_performance = results.groupby('config_feature_set')['metric_test_mae'].mean().sort_values()\n",
    "        if len(fs_performance) > 0:\n",
    "            best_fs = fs_performance.index[0]\n",
    "            print(f\"\\n‚úÖ Best Feature Set: {best_fs}\")\n",
    "            print(f\"   Average Test MAE: {fs_performance.iloc[0]:.4f}\")\n",
    "    \n",
    "    print(\"\\nüìä Next Steps:\")\n",
    "    print(\"   1. Retrain best models on full dataset\")\n",
    "    print(\"   2. Perform per-user error analysis\")\n",
    "    print(\"   3. Investigate feature importance for best models\")\n",
    "    print(\"   4. Test on holdout set or new data\")\n",
    "    print(\"   5. Consider ensemble of top models\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No results available. Run hyperparameter search first.\")\n",
    "    print(\"   python run_hyperparameter_search.py --mode quick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
